---
title: "Meeting 7 - Distributions and Parameter Estimation"
author: "Matthew E. Aiello-Lammens"
output:
  html_document:
    toc: yes
    code_folding: hide
---

# Important properties of distributions

## Measures of location, dispersion, and variability 

When we are looking at the distribution of any dataset, we want to have a measure of the center of the distribution.
Usually our first step will be to look at the **arithmetic mean**, but each statistical distribution has its own **Expected Value**.

Similarly, we want to have a measure of dispersion and variability. 
Again, we usually would do something like calculate the **standard deviation**, but each distribution has its own measure of **Variance**.


### Terms and properties of distributions

Consider a generic probability distribution, with a PDF of 

$$
P( a < x < b) = \int_a^b f(x|params)dx
$$

(or an analygous PMF). This distribution has a number of properties that could be described.

* Parameters
    + Location - where
    + Shape - eponymous
    + Scale - spread
* Support - what values of $x$ are possible
* PDF or PMF
* Mean - expected value
* Median - middle value
* Mode - most common value
* Variance
* Skewness - measure of asymmetry
* Kurtosis - how fat/thin the tails are


## Estimating distribution parameters
 
**What are we estimating?**

Usually measures of **location** and **dispersion and variability**.

#### Challenge

Can you think of any measures of location and dispersion that you are familiar with?

### Expected value of a distribution

* Long-run average value
* Mean

### Expected value for discrete distributions

$$
E[X] = \sum_{i=1}^{\infty} x_i p_i
$$

### Expected value for continuous distributions

$$
E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx
$$

### Population versus sample statistics

We use **sample statistics** to estimate **population statistics**.
In most cases in biology, populations are too large to measure population parameters directly.
Therefore, we use different **estimators** to calculate the **populations statistics** based on the **sample statistics**.


### Properities of good estimators

See Q&K page 15 for more details.

1. Unbiased - repeated estimates should not consistently under- or over-estimate population parameters
2. Consistent - as sample size increases, sample estimate should get closer to population estimate
3. Efficient - estimate has lowest variance among other estimators

#### Challenge 

What is the difference between accuracy vs. precision[^1]


[^1]:https://en.wikipedia.org/wiki/Accuracy_and_precision


### Different kinds of estimators

* **Point estimate** - a single value estimate for a population parameter
* **Interval estimate** - a range of values that might include the parameter with a known probability

#### Challenge

Give an example of a point estimate and an interval estimate.

***

# Other important distributions we'll discuss in this class

* Standard Normal
* Student's $t$ distribution
* $\chi^2$ distribution
* $F$ distribution


## Standard Normal distribution

Recall that in [Lecture 6](http://mlammens.github.io/ENS-623-Research-Stats/lectures/Lecture-6.html#normal-distribution) we were introduced to the Normal (or Gaussian) Distribution with pdf:

$$
f(x|\mu,\sigma) \thicksim \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

The **Standard Normal** distribution is a **Normal** distribution with $\mu = 0$ and $\sigma = 1$.

#### Challenge

Draw 1000 samples from the standard normal distribution and plot a histogram and density curve for the resulting data.
After making this plot, try 10,000 samples.

```{r}
stdnorm_samps <- data.frame( samps = rnorm(n = 10000, mean = 0, sd = 1) )

library(ggplot2)

ggplot(data = stdnorm_samps, aes(x = samps)) +
  geom_histogram( aes( y = ..density.. ) ) +
  geom_density(colour = "red", size = 2) + 
  theme_bw()
```

* How well does the density plot represent the standard normal distribution?
* Can we add the normal distribution?

```{r, fig.show='hide'}
ggplot(data = stdnorm_samps, aes(x = samps)) +
  geom_histogram( aes( y = ..density.. ) ) +
  geom_density(colour = "red", size = 2) + 
  stat_function( fun = dnorm, colour = "blue", alpha = 0.6, size = 2) +
  theme_bw()
```


We can convert from a normal distribution with any mean $\mu$ and standard deviation $\sigma$ to the standard normal by subtracting the mean from each value, then dividing by the standard deviation:

$$
z_i = \frac{y_i - \mu}{\sigma}
$$

### The utility of knowing the amount of area under the curve for probability densities

* Where do 95% of the values from the standard normal fall? **Introducing `qnorm`.**

```{r}
qnorm(.025)
qnorm(.975)
```

* How might we visualize this?

```{r}
ggplot(data = stdnorm_samps, aes(x = samps)) +
  coord_cartesian(xlim = c(-5, 5), ylim = c(0, 0.41)) +
  geom_vline(xintercept = c(-1.96, 1.96), size = 2 ) +
  stat_function( fun = dnorm, colour = "blue", size = 2) +
  theme_bw()
```


# The Central Limit Theorem

This section gets into more nitty-gritty probability and statistics. 
You should review this information in both of the course text books.

## Sampling distribution of the mean

The frequency distribution of the **mean values** of several samples taken from a population.

From Q&K p. 18:

> The probability distribution of means of samples from a normal distribution is also normally distributed.

#### Challenge

1. Take 100 samples of 10 observations from a normal distribution with a mean of 0 and a standard deviation of 1. Plot a histogram of the means.
2. Increase the number of samples. How does this affect the outcome?
3. Increase the number of observations. How does this affect the outcome?

```{r}


```


## CLT

In the limit as $n \to \infty$, the probability distribution of means of samples from ***any distribution*** will approach a normal distribution.

#### Challenge

First, plot a histogram of 100 observations that are **Poisson distributed** with a $\lambda$ value $= 1$.
Describe the shape of the distribution. 
Does it look normal?

```{r, fig.show='hide'}
ggplot(data = NULL) +
  geom_histogram(aes(x = rpois(100, lambda = 1)))
```


Next, take 100 samples of 10 observations from a **Poisson distribution**, calculate the mean for each sample. Plot a histogram of the means.

```{r}

```

## Using the CLT

From Q&K p. 18:

> The expected value or mean of the probability distribution of sample means equals the mean of the population ($\mu$) from which the samples were taken.


### Standard error of the sample mean

We just demonstrated that **sample means are normally distributed**. 
What can we do with this?
To start, we can say something about the precision and accuracy of our estimates of the population mean based on the sample mean.

**Standard error of the mean** - the expected value of the standard deviation of the sample mean:

$$
\sigma_{\bar{y}} = \frac{\sigma}{\sqrt{n}}
$$

Often times we only have **one sample** from the population. What can we say then?

Standard error of the mean:

$$
s_{\bar{y}} = \frac{s}{\sqrt{n}}
$$

where $s$ is the sample estimate of the standard deviation of the original population and $n$ is the sample size.

**NB:** The standard error of the means "tells us about the error in using $\bar{y}$ to estimate $\mu$."


## Confidence intervals for the population mean

Recall that we can convert any observation from normal distribution to it's equivalent value from a standard normal distribution using:

$$
z_i = \frac{y_i - \mu}{\sigma}
$$

These are sometimes called ***z-scores***.

Using this formula, we can convert a sample mean, $\bar{y}$, into its corresponding value from a standard normal using:

$$
z = \frac{\bar{y} - \mu}{\sigma_{\bar{y}}}
$$

where the denominator is the standard error of the mean (see above). 

We can use this information to estimate how confident we are that are sample mean is a good representation of the *true* population mean.
Again, we are doing this by taking advantage of the fact that we know the sample means are *normally distributed*.
We calculate the range in which 95% of the sample mean values fall.

In mathematical terms, this looks like:

$$
P\{\bar{y} - 1.96\sigma_{\bar{y}} \leq \mu \leq \bar{y} + 1.96\sigma_{\bar{y}} \} = 0.95
$$

**VERY IMPORTANT:** The probability statement here is about the *interval*, not $\mu$. 
$\mu$ is not a random variable; it is fixed.
What we are saying is that there is 95% confidence that this interval includes the population mean, $\mu$.

#### Major Challenge

Generate 100 samples from a standard normal distribution of 10 observations each.
Calculate the mean and confidence interval for each. 
Plot each mean and confidence interval, using the `geom_errorbar` function in `ggplot2`.

```{r}

```


### Unknown $\sigma$ (and $\sigma_{\bar{y}}$)

We rarely know $\sigma$, so we cannot calculate $\sigma_{\bar{y}}$. 
So what *can* we do?

Instead, we use the sample standard deviation, $s_{\bar{y}}$. 
So now we are dealing with a random variable that looks like: 

$$
\frac{\bar{y}-\mu}{s_{\bar{y}}}
$$

which is no longer standard normal distributed, but rather ***t* distributed**.

#### Challenge

Use whatever resource available to you, and look-up the ***t* distribution**.

#### Challenge

What is/are the parameter(s) of the *t* distribution?


### Degrees of freedom

> The number of observations in our sample that are "free to vary" when we are estimating the variance (Q&K p.20; Box 2.1).

If I have calculated the mean, how many observations, out of a total of *n*, are free to vary?

**Answer:** $n-1$. Once I have the mean, I can use it and any $n-1$ values to calculate the *n*th value.

General rule regarding df's:

> the df is the number of observations minus the number of parameters included in the formula for the variance (Q&K p.20; Box 2.1).

