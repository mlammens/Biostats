---
title: "Lecture 5"
author: "Matthew E. Aiello-Lammens"
date: "February 23, 2016"
output: html_document
---

## Standard error of other statistics

> The standard error is ... the standard deviation of the probability distribution of a specific statistics (Q&K p. 20).

### Standard error of the sample variance

The distribution of the sample variance is *not normal*. It is $\chi^2$ distributed.

Mathematically, this is explained as:

$$
X \thicksim N(0,1) \to X^2 \thicksim \chi^2
$$

Intuitively, why does this make sense?

# Parameter estimation revisited

There are two common methods used to estimate distribution parameters. 
**Ordinary least-squares (OLS)** and **maximum likelihood (ML)**.
Let's start by introducing OLS.

## Ordinary least-squares

Minimize the observed differences between sample values and the estimated parameter(s).

### Example - least squares estimate of the population mean

> [T]he least squares estimate of the population mean is a value that minimizes the differences between the sample values and this estimated mean (Logan p. 73).

#### Challenge

Work through an example of the least squares estimate of a population mean.
Start with the pseudo-code.

### Example - linear regression

Briefly mention this example, but we will return to it in the future.

## Maximum-likelihood

The ML approach estimates population parameters by **maximizing** the (log) likelihood of obtaining the observed sample values, assuming you have knowledge of what the probability distribution is.

### Relationship of *PDF* and *Likelihood*

*PDF:* Parameters are known, data varies
*Likelihood:* Data are known, parameters vary

**We will continue working with maximum likelihood next week.**


## Teaser for next week

Let's say we have a sample of observations that we want to know if they come from some population with a known value for $\mu$. 
We can see how (un)likely it is that the sample estimates come from this particular population, by looking at where these values fall in a *t* distribution. That is, calculate the *t* statistic:

$$
t_s = \frac{St - \theta}{S_{St}}
$$

# Relationships among the distributions

![distribution relationships](/Volumes/Garage/Professional-Annex/SBU-Grad-School/SBU-Teaching/Biometry-BEE-552-SP13/Figure adapted from Leemis (1986).pdf) 

