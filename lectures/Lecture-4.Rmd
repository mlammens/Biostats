---
title: "Lecture 4 - Distributions, Estimation, and Hypothesis Testing"
author: "Matthew E. Aiello-Lammens"
output:
  html_document:
    toc: yes
---

# Review of distributions

#### Challenge 

What is a probability distribution? 

## Important properties of distributions

* Parameters
    + Location - where
    + Shape - eponymous
    + Scale - spread
* Support - what values of $x$ are possible
* PDF or PMF
* Mean - expected value
* Median - middle value
* Mode - most common value
* Variance
* Skewness - measure of asymmetry
* Kurtosis - how fat/thin the tails are

#### Challenge

Describe the difference between empirical probability distributions versus statistical probability distributions.

#### Challenge

Describe the differences between continuous and discrete probability distributions.


## Estimating distribution parameters
 
**What are we estimating?**

Usually measures of **location** and **dispersion and variability**.

#### Challenge

What measures did you use in the homework?

### Expected value of a distribution

* Long-run average value
* Mean

#### Expected value for discrete distributions

$$
E[X] = \sum_{i=1}^{\infty} x_i p_i
$$

#### Expected value for continuous distributions

$$
E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx
$$

### Population versus sample statistics

We use **sample statistics** to estimate **population statistics**.
In most cases in biology, populations are too large to measure population parameters directly.
Therefore, we use different **estimators** to calculate the **populations statistics** based on the **sample statistics**.

### Properities of good estimators

See Q&K page 15 for more details.

1. Unbiased - repeated estimates should not consistently under- or over-estimate population parameters
2. Consistent - as sample size increases, sample estimate should get closer to population estimate
3. Efficient - estimate has lowest variance among other estimators

### Types of estimators

* **Point estimate** - a single value estimate for a population parameter
* **Interval estimate** - a range of values that might include the parameter with a known probability

#### Challange

Give an example of a point estimate and an interval estimate.

## Accuracy and precision

What are they?



## Other important distributions we'll discuss

* Standard Normal
* Student's $t$ distribution
* $\chi^2$ distribution
* $F$ distribution


### Standard Normal distribution

Recall that in Lecture 3 we were introduced to the Normal (or Gaussian) Distribution with pdf:

$$
f(x|\mu,\sigma) ~ \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

The **Standard Normal** distribution is one in which $\mu = 0$ and $\sigma = 1$.

#### Challenge

Draw 1000 samples from the standard normal distribution and plot a histogram and density curve for the resulting data.
After making this plot, try 10,000 samples.

```{r}
stdnorm_samps <- data.frame( samps = rnorm(n = 10000, mean = 0, sd = 1) )

library(ggplot2)

ggplot(data = stdnorm_samps, aes(x = samps)) +
  geom_histogram( aes( y = ..density.. ) ) +
  geom_density(colour = "red", size = 2) + 
  stat_function( fun = dnorm, colour = "blue", alpha = 0.6, size = 2) +
  theme_bw()
```

* How well does the density plot represent the standard normal distribution?
* Can we add the normal distribution?

We can convert from a normal distribution with some mean $\mu$ and standard deviation $\sigma$ to the standard normal by subtracting the mean from each value, then dividing by the standard deviation:

$$
z = \frac{y_i - \mu}{\sigma}
$$

#### Utility of knowing the amount of area under the curve for probability densities

* Where do 95% of the values from the standard normal fall?

```{r}
qnorm(.025)
qnorm(.975)
```

```{r}
ggplot(data = stdnorm_samps, aes(x = samps)) +
  coord_cartesian(xlim = c(-5, 5), ylim = c(0, 0.41)) +
  geom_vline(xintercept = c(-1.96, 1.96), size = 2 ) +
  stat_function( fun = dnorm, colour = "blue", size = 2) +
  theme_bw()
```


# The Central Limit Theorem

## Sampling distribution of the mean

The frequency distribution of the **mean values** of several samples taken from a population.

From Q&K p. 18:

> The probability distribution of means of samples from a normal distribution is also normally distributed.

#### Challenge

1. Take 100 samples of 10 observations from a normal distribution with a mean of 0 and a standard deviation of 1. Plot a histogram of the means.
2. Increase the number of samples. How does this affect the outcome?
3. Increase the number of observations. How does this affect the outcome?

```{r}

```


## CLT

In the limit as $n \to \infty$, the probability distribution of means of samples from ***any distribution*** will approach a normal distribution.

#### Challenge

Take 100 samples of 10 observations from a **Poisson distribution**, calculate the mean for each sample. Plot a histogram of the means.

```{r}

```

## Using the CLT

From Q&K p. 18:

> The expected value or mean of the probability distribution of sample means equals the mean of the population ($\mu$) from which the samples were taken.


### Standard error of the sample mean

Know we know that sample means are normally distributed. What can we do with this?
To start, we can say something about the precision and accuracy of our estimates of the population mean based on the sample mean.

**Standard error of the mean** - the expected value of the standard deviation of the sample mean:

$$
\sigma_{\bar{y}} = \frac{\sigma}{\sqrt{n}}
$$

Often times we only have on sample from the population. What can we say then?

Standard error of the mean:

$$
s_{\bar{y}} = \frac{s}{\sqrt{n}}
$$

where $s$ is the sample estimate of the standard deviation of the original population and $n$ is the sample size.

**NB:** The standard error of the means "tells us about the error in using $\bar{y}$ to estimate $\mu$.


## Confidence intervals for the population mean

Recall that we can convert any from a normal distribution to it's equivalent value from a standard normal distribution using:

$$
z = \frac{y_i - \mu}{\sigma}
$$

These are sometimes called ***z-scores***.

Using this formula, we can convert a sample mean, $\bar{y}$, into its corresponding value from a standard normal using:

$$
z = \frac{\bar{y} - \mu}{\sigma_{\bar{y}}}
$$

where the denominator is the standard error of the mean (see above). 

We can use this information to estimate how confident we are that are sample mean is a good representation of the *true* population mean.
Again, we're doing this by taking advantage of the fact that we know the sample means are normally distributed.
We calculate the range in which 95% of the sample mean values fall.

In mathematical terms, this looks like:

$$
P\{\bar{y} - 1.96\sigma_{\bar{y}} \leq \mu \leq \bar{y} + 1.96\sigma_{\bar{y}} \} = 0.95
$$

**VERY IMPORTANT:** The probability statement here is about the interval, not $\mu$. 
$\mu$ is not a random variable; it is fixed.
What we are saying is that there is 95% confidence that this interval includes the population mean, $\mu$.

#### Major Challenge

Generate 100 samples from a standard normal distribution of 10 observations each.
Calculate the mean and confidence interval for each. 
Plot each mean and confidence interval, using the `geom_errorbar` function in `ggplot2`.

### Unknown $\sigma$ (and $\sigma_{\bar{y}}$)

We rarely know $\sigma$, so we cannot calculate $\sigma_{\bar{y}}$. What can we do?

Instead, we use the sample standard deviation, $s_{\bar{y}}$. 
So know we are dealing with a random variable that looks like: 

$$
\frac{\bar{y}-\mu}{s_{\bar{y}}}
$$

which is no longer standard normal distributed, but rather ***t* distributed**.

#### Challenge

Use whatever resource available to you, and look-up the ***t* distribution**.

#### Challenge

What is/are the parameter(s) of the *t* distribution?

### Degrees of freedom

> The number of observations in our sample that are "free to vary" when we are estimating the variance (Q&K p.20; Box 2.1).

If I have calculated the mean, how many observations, out of a total of *n*, are free to vary?

**Answer:** $n-1$. Once I have the mean, I can use it and any $n-1$ values to calculate the *n*th value.

General rule regarding df's:

> the df is the number of observations minus the number of parameters included in the formula for the variance (Q&K p.20; Box 2.1).

## Standard error of other statistics

> The standard error is ... the standard deviation of the probability distribution of a specific statistics (Q&K p. 20).

### Standard error of the sample variance

The distribution of the sample variance is *not normal*. It is $\chi^2$ distributed.

Mathematically, this is explained as:

$$
X \thicksim N(0,1) \to X^2 \thicksim \chi^2
$$

Intuitively, why does this make sense?

# Parameter estimation revisited

There are two common methods used to estimate distribution parameters. 
**Ordinary least-squares (OLS)** and **maximum likelihood (ML)**.
Let's start by introducing OLS.

## Ordinary least-squares

Minimize the observed differences between sample values and the estimated parameter(s).

### Example - least squares estimate of the population mean

> [T]he least squares estimate of the population mean is a value that minimizes the differences between the sample values and this estimated mean (Logan p. 73).

#### Challenge

Work through an example of the least squares estimate of a population mean.
Start with the pseudo-code.

### Example - linear regression

Briefly mention this example, but we will return to it in the future.

## Maximum-likelihood

The ML approach estimates population parameters by **maximizing** the (log) likelihood of obtaining the observed sample values, assuming you have knowledge of what the probability distribution is.

### Relationship of *PDF* and *Likelihood*

*PDF:* Parameters are known, data varies
*Likelihood:* Data are known, parameters vary

**We will continue working with maximum likelihood next week.**


## Teaser for next week

Let's say we have a sample of observations that we want to know if they come from some population with a known value for $\mu$. 
We can see how (un)likely it is that the sample estimates come from this particular population, by looking at where these values fall in a *t* distribution. That is, calculate the *t* statistic:

$$
t_s = \frac{St - \theta}{S_{St}}
$$

# Relationships among the distributions

![distribution relationships](/Volumes/Garage/Professional-Annex/SBU-Grad-School/SBU-Teaching/Biometry-BEE-552-SP13/Figure adapted from Leemis (1986).pdf) 
