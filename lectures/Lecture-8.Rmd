---
title: "Meeting 8 - Parameter Estimation and Hypothesis Testing"
author: "Matthew E. Aiello-Lammens"
output:
  html_document:
    toc: yes
    code_folding: hide
---

# Refresh

What is a statistical distribution?

What are some of the distributions we talked about in the past?

What characteristics of distributions are we often interested in?


# Other important distributions in data analysis

* Standard Normal
* Student's $t$ distribution
* $\chi^2$ distribution
* $F$ distribution


## Standard Normal distribution

Recall that in [Lecture 6](http://mlammens.github.io/ENS-623-Research-Stats/lectures/Lecture-6.html#normal-distribution) we were introduced to the Normal (or Gaussian) Distribution with pdf:

$$
f(x|\mu,\sigma) \thicksim \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

The **Standard Normal** distribution is a **Normal** distribution with $\mu = 0$ and $\sigma = 1$.

#### Challenge

Draw 1000 samples from the standard normal distribution and plot a histogram and density curve for the resulting data.
After making this plot, try 10,000 samples.

```{r}
stdnorm_samps <- data.frame( samps = rnorm(n = 10000, mean = 0, sd = 1) )

library(ggplot2)

ggplot(data = stdnorm_samps, aes(x = samps)) +
  geom_histogram( aes( y = ..density.. ) ) +
  geom_density(colour = "red", size = 2) + 
  theme_bw()
```

* How well does the density plot represent the standard normal distribution?
* Can we add the functional form of the normal distribution?

```{r, fig.show='hide'}
ggplot(data = stdnorm_samps, aes(x = samps)) +
  geom_histogram( aes( y = ..density.. ) ) +
  geom_density(colour = "red", size = 2) + 
  stat_function( fun = dnorm, colour = "blue", alpha = 0.6, size = 2) +
  theme_bw()
```


We can convert from a normal distribution with any mean $\mu$ and standard deviation $\sigma$ to the standard normal by subtracting the mean from each value, then dividing by the standard deviation:

$$
z_i = \frac{y_i - \mu}{\sigma}
$$

### The utility of knowing the amount of area under the curve for probability densities

* Where do 95% of the values from the standard normal fall? **Introducing `qnorm`.**

```{r}
qnorm(.025)
qnorm(.975)
```

* How might we visualize this?

```{r}
ggplot(data = stdnorm_samps, aes(x = samps)) +
  coord_cartesian(xlim = c(-5, 5), ylim = c(0, 0.41)) +
  geom_vline(xintercept = c(-1.96, 1.96), size = 2 ) +
  stat_function( fun = dnorm, colour = "blue", size = 2) +
  theme_bw()
```


# The Central Limit Theorem

This section gets into more nitty-gritty probability and statistics. 
You should review this information in both of the course text books.

## Sampling distribution of the mean

The frequency distribution of the **mean values** of several samples taken from a population.

From Q&K p. 18:

> The probability distribution of means of samples from a normal distribution is also normally distributed.

#### Challenge

1. Take 100 samples of 10 observations from a normal distribution with a mean of 0 and a standard deviation of 1. Plot a histogram of the means.
2. Increase the number of samples. How does this affect the outcome?
3. Increase the number of observations. How does this affect the outcome?

```{r}


```


## CLT

In the limit as $n \to \infty$, the probability distribution of means of samples from ***any distribution*** will approach a normal distribution.

#### Problem Set 6 - review

Plot a histogram of 100 observations that are **Poisson distributed** with a $\lambda$ value $= 1$.
Describe the shape of the distribution. 
Does it look normal?

```{r, fig.show='hide'}
ggplot(data = NULL) +
  geom_histogram(aes(x = rpois(100, lambda = 1)))
```


Next, take 100 samples of 10 observations from a **Poisson distribution**, calculate the mean for each sample. Plot a histogram of the means.

```{r}

```

## Using the CLT

From Q&K p. 18:

> The expected value or mean of the probability distribution of sample means equals the mean of the population ($\mu$) from which the samples were taken.


### Standard error of the sample mean

We just demonstrated that **sample means are normally distributed**. 
What can we do with this?
To start, we can say something about the precision and accuracy of our estimates of the *population* mean based on the *sample* mean.

**Standard error of the mean** - the expected value of the standard deviation of the sample mean:

$$
\sigma_{\bar{y}} = \frac{\sigma}{\sqrt{n}}
$$

Often times we only have **one sample** from the population. What can we say then?

Standard error of the mean:

$$
s_{\bar{y}} = \frac{s}{\sqrt{n}}
$$

where $s$ is the sample estimate of the standard deviation of the original population and $n$ is the sample size.

**NB:** The standard error of the means "tells us about the error in using $\bar{y}$ to estimate $\mu$."


## Confidence intervals for the population mean

Recall that we can convert any observation from normal distribution to it's equivalent value from a standard normal distribution using:

$$
z_i = \frac{y_i - \mu}{\sigma}
$$

These are sometimes called ***z-scores***.

Using this formula, we can convert a sample mean, $\bar{y}$, into its corresponding value from a standard normal using:

$$
z = \frac{\bar{y} - \mu}{\sigma_{\bar{y}}}
$$

where the denominator is the standard error of the mean (see above). 

We can use this information to estimate how confident we are that are sample mean is a good representation of the *true* population mean.
Again, we are doing this by taking advantage of the fact that we know the sample means are *normally distributed*.
We calculate the range in which 95% of the sample mean values fall.

In mathematical terms, this looks like:

$$
P\{\bar{y} - 1.96\sigma_{\bar{y}} \leq \mu \leq \bar{y} + 1.96\sigma_{\bar{y}} \} = 0.95
$$

**VERY IMPORTANT:** The probability statement here is about the *interval*, not $\mu$. 
$\mu$ is not a random variable; it is fixed.
What we are saying is that there is 95% confidence that this interval includes the population mean, $\mu$.

#### Major Challenge

Generate 100 samples from a standard normal distribution of 10 observations each.
Calculate the mean and confidence interval for each. 
Plot each mean and confidence interval, using the `geom_errorbar` function in `ggplot2`.

```{r}

```


### Unknown $\sigma$ (and $\sigma_{\bar{y}}$)

We rarely know $\sigma$, so we cannot calculate $\sigma_{\bar{y}}$. 
So what *can* we do?

Instead, we use the sample standard deviation, $s_{\bar{y}}$. 
So now we are dealing with a random variable that looks like: 

$$
\frac{\bar{y}-\mu}{s_{\bar{y}}}
$$

which is no longer standard normal distributed, but rather ***t* distributed**.

#### Challenge

Use whatever resource available to you, and look-up the ***t* distribution**.
What is/are the parameter(s) of the *t* distribution?


### Degrees of freedom

> The number of observations in our sample that are "free to vary" when we are estimating the variance (Q&K p.20; Box 2.1).

If I have calculated the mean, how many observations, out of a total of *n*, are free to vary?

**Answer:** $n-1$. Once I have the mean, I can use it and any $n-1$ values to calculate the *n*th value.

General rule regarding df's:

> the df is the number of observations minus the number of parameters included in the formula for the variance (Q&K p.20; Box 2.1).


## Standard error of other statistics

> The standard error is ... the standard deviation of the probability distribution of a specific statistics (Q&K p. 20).

### Standard error of the sample variance

The distribution of the sample variance is *not normal*. It is $\chi^2$ distributed.

Mathematically, this is explained as:

$$
X \thicksim N(0,1) \to X^2 \thicksim \chi^2
$$

Intuitively, why does this make sense?

# Parameter estimation revisited

There are two common methods used to estimate distribution parameters. 
**Ordinary least-squares (OLS)** and **maximum likelihood (ML)**.
Let's start by introducing OLS.

## Ordinary least-squares

Minimize the observed differences between sample values and the estimated parameter(s).

### Example - least squares estimate of the population mean

> [T]he least squares estimate of the population mean is a value that minimizes the differences between the sample values and this estimated mean (Logan p. 73).

#### Challenge

Work through an example of the least squares estimate of a population mean.
Start with the pseudo-code.

### Example - linear regression

Briefly mention this example, but we will return to it in the future.

***

## Maximum-likelihood - NOTE: Maximum-likelihood will be discussed in a video supplement

The ML approach estimates population parameters by **maximizing** the (log) likelihood of obtaining the observed sample values, assuming you have knowledge of what the probability distribution is.

### Likelihood

Maximum likelihood and probability distributions are intimately related, for reasons that will become apparent. 
To serve as an example, we'll use the Normal Distribution $N(\mu,\sigma^{2})$:

The probability density of the normal distribution is given by

$$
f(x|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(x-\mu)^{2}}{\sigma^{2}}\right)}
$$

For variables that are **independent and identically distributed** (i.i.d.), the joint probability $(X_{1},X_{2})$ is simply the product of the two p.d.f.s

$$
P(A\cap B \cap C)=P(A)\times P(B)\times P(C)
$$

Extending this to the case of several random variables drawn from a normal distribution, we can write:

$$
f(X_{1},X_{2},...,X_{n}|\mu, \sigma) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)}
$$

Taken as a probability density, this equation denotes the probability of getting unknown data ${X_{1},X_{2},...,X_{n}}$ given (|) the known distribution parameters $\mu$ and $\sigma$. 

However, this can be rewritten as a likelihood simply by reversing the conditionality:

$$
L(|\mu, \sigma) = \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)}
$$

The likelihood specifies the probability of obtaining the known data ${X_{1},X_{2},...,X_{n}}$ by a certain combination of the unknown parameters $\mu$ and $\sigma$.

### Relationship of *PDF* and *Likelihood*

**PDF:** Parameters are known, data varies  
**Likelihood:** Data are known, parameters vary


### Maximizing the Likelihood

Parameter estimates may be found by maximum likelihood simply by finding those parameters that make your data most likely (among all possible data sets).

Conceptually, it helps to remember our examples from working with M&Ms. 
The likelihood of obtaining your exact set of colors was very small even when using the true underlying probabilities of each color. 
Likelihoods are always VERY SMALL - even the maximum likelihood estimates (MLEs) are very unlikely to produce your dataset, simply because there are so many possible datasets that could be produced. 
The MLEs are simply those parameters that make your dataset more likely than any other dataset.

The magnitude of the likelihood means NOTHING. 
The actual value of the likelihood depends on the size of the “sample space” (how many possible datasets could you imagine getting?), so we can only assign meaning to the relative size of likelihoods among different combinations of parameter values. 
We can say whether one set of parameter values is more likely to be the “true” population values than other possible sets of parameter values.

So how do we find the MLEs?

First we will calculate the MLE for the normal parameters by hand, and then we will use two different methods of calculating the maximum likelihood estimators using R. 

#### Manual calculation of Maximum Likelihood

The likelihood function for X drawn from $N(\mu,\sigma^{2})$ is:

$$
L(\mu,\sigma|X_{1},X_{2},...,X_{n})= \prod^{n}_{i=1}\frac{1}{\sqrt{2\pi\sigma^{2}}} \exp{\left(-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)}
$$

Because likelihoods are very small, and we are only interested in relative values, we use the log-likelihood values which are easier to work with (for reasons that will become clear).

The log-likelihood (LL) is:

$$
LL = \sum_{i}\left(-\frac{1}{2}log(2\pi\sigma^{2})-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\sigma^{2}}\right)
$$

We want to maximize the LL, which is usually done by minimizing the negative-LL (NLL).
To make the algebra easier, we will define $A=\sigma^{2}$.

$$
NLL=\sum_{i}\left(\frac{1}{2}log(2\pi A)+\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{A}\right)
$$

In order to find the minimum negative log-likelihood value, we must first take the partial derivative of the above equation and set it to 0, as:

$$
\frac{\partial NLL}{\partial \mu} = \sum_{i}\left(\frac{-(X_{i}-\hat{\mu})}{\sigma^{2}}\right)=0
$$

Notice that when I set the left-hand side to 0, the notation changes from $\mu$ to $\hat{\mu}$ because the MLE $\hat{\mu}$ is that value that makes that statement true. Solving for $\hat{\mu}$, we get:

$$
\frac{\partial NLL}{\partial \mu} =\sum_{i}-(X_{i}-\hat{\mu})=0=\Sigma_{i}(X_{i}-\hat{\mu})
$$

$$
n\hat{\mu}-\sum_{i}X_{i}=0
$$

$$
\hat{\mu}=\frac{1}{n}\sum_{i}X_{i}
$$

Now we do the same for $A=\sigma^{2}$

$$
NLL=\sum_{i}\left(\frac{1}{2}log(2\pi A)+\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{A}\right)
$$

$$
\frac{\partial NLL}{\partial A} = \sum_{i}\left(\frac{1}{2}\frac{2\pi}{2\pi\hat{A}}-\frac{1}{2}\frac{(X_{i}-\mu)^{2}}{\hat{A}^{2}}\right)=0
$$

$$
\sum_{i}\left(1-\frac{(X_{i}-\mu)^{2}}{\hat{A}}\right)=0
$$

$$
n-\frac{1}{\hat{A}}\sum_{i}\left((X_{i}-\mu)^{2}\right)=0
$$

$$
\hat{A}=\hat{\sigma^{2}}=\frac{1}{n}\sum_{i}(X_{i}-\mu)^{2}
$$

**NOTE** that the MLEs are not necessarily the best estimates, or even unbised estimates. 
In this case the MLE for $\sigma^{2}$ is biased (the unbiased estimator replaces n with n-1).

#### R based estimate of Maximum Likelihoods - 1

To estimate the Maximum Likelihood values in R, we first *write a function to define the NLL*:

```{r}
neg.ll <- function(x, mu, sigma2) {
    sum(0.5 * log(2 * pi * sigma2) + 0.5 * ((x - mu)^2)/sigma2)
}
```

For the purposes of a simple exmaple, lets generate some fake “data” by drawing random samples from a $N(\mu=1,\sigma=2)$.

```{r}
x <- rnorm(1000, mean = 1, sd = 2)
mu.test.values <- seq(-2, 4, 0.1)
sigma2.test.values <- seq(1, 11, 0.1)
```

Next, we will make a matrix to store the values of the likelihood for a grid of potential $\mu$ and $\sigma^{2}$ values.

```{r}
likelihood.matrix <- matrix(nrow = length(mu.test.values), ncol = length(sigma2.test.values))
```


Now we will search parameter space by brute force, calculating the likelihood on a grid of potential $\mu$ and $\sigma^{2}$ values.

```{r}
for (i in 1:length(mu.test.values)) {
    for (j in 1:length(sigma2.test.values)) {
        likelihood.matrix[i, j] <- neg.ll(x, mu.test.values[i], sigma2.test.values[j])
    }
}
```

We can plot the results using the functions `image` and `contour`, and place on top of this plot the maximum likelihood as found by the grid search as well as the known parameter values.

```{r}
image(mu.test.values, sigma2.test.values, likelihood.matrix, col = topo.colors(100))
contour(mu.test.values, sigma2.test.values, likelihood.matrix, nlevels = 30, 
    add = T)

max.element <- which(likelihood.matrix == min(likelihood.matrix), arr.ind = T)
points(mu.test.values[max.element[1]], sigma2.test.values[max.element[2]], pch = 16, 
    cex = 2)
points(1, 4, pch = "x", cex = 2)
```

Now we can plot the likelihood “slices”, which show cross sections across the search grid for fixed values of $\mu$ or $\sigma^{2}$.

```{r}
par(mfrow = c(1, 2))
plot(mu.test.values, likelihood.matrix[, max.element[2]], typ = "b")
plot(sigma2.test.values, likelihood.matrix[max.element[1], ], typ = "b")
```

#### R based estimate of Maximum Likelihoods - 2

R has a function 'optim' which optimizes functions (and is thus much better than a simple grid search 'brute force' approach we just did) and is very handy for minimizing the LL. 
We take advantage of the R function that gives us the probability density function, which saves us having to hard code that into R. 
Make sure the use of `dnorm` in the code below makes sense!

```{r}
neg.ll.v2 <- function(x, params) {
    mu = params[1]
    sigma = params[2]
    -sum(dnorm(x, mean = mu, sd = sigma, log = TRUE))
}
```

Notice that we are using the “log-TRUE” option to take the log inside the `dnorm` command, which saves us taking it later. 
We also had to pass the parameters as one variable since that is what `optim` is expecting. 
Take a second to convince yourself that the `neg.ll` and `neg.ll.v2` functions give the same answer.

We still need a way to maximize the log-likelihood and for this we use the function `optim`:

```{r}
opt1 <- optim(par = c(1, 1), fn = neg.ll.v2, x = x)
opt1
```

An even easier way is to use the `fitdistr` command, but the `optim` function comes in handy all the time and is the only option you have when fitting non-tranditional distributions not covered by `fitdistr`.

```{r}
library(MASS)
fitdistr(x, "normal")
```

Notice that this function outputs the standard errors of the MLE values as well, whereas our function and `optim` only give the MLE.

***

# Hypothesis Testing

* What is a hypothesis? 
* How do we test our hypothesis?

The answers to these questions, with respect to this course, are primarily based on frequentist statistics.


## Example

Let's say we have a sample of observations that we want to know if they come from some population with a known value for $\mu$ (i.e., we know the population mean). 
We can see how (un)likely it is that the sample estimates come from this particular population, by looking at where these values fall in a *t* distribution. That is, calculate the *t* statistic:

$$
t_s = \frac{St - \theta}{S_{St}}
$$


## *t*-test

* Looking at differences between two samples of data. 
The *differences* should be *t* distributed.
* **Major assumptions** - the samples are drawn from populations that are:
    1. Normally Distributed
    2. Equally varied (i.e. equal variance)
    3. Each observation is *independent*
    
## We select the error rate

General concencus is $p = 0.05$. This is known as Type-I error, or $\alpha$.

### Type-I versus Type-II error

* **Type-I error: $\alpha$** - our test suggests there is an effect, but there really is not one
* **Type-II error: $\beta$** - when you fail to detect an effect that really occurs

### Statistical power

The reciprical of Type-II error ($\beta$) is **power**. 

$$
power(1-\beta) \propto \frac{ES \sqrt{n} \alpha}{\sigma}
$$

#### How do we increase statistical power?

Increase the sample size.

* Distribution of the mean becomes narrower
* Acceptance region becomes narrower
* Curves overlap less
* Type II error rate becomes smaller

![power](https://www.dropbox.com/s/joh0kxc7ygsq9xw/power.png?dl=1)



# Attributions

* Much of the Maximum Likelihood section of this lecture is adapted from material developed by Prof. Heather Lynch at Stony Brook University for teaching BEE 552: Biometry.
