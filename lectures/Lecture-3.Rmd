---
title: "Lecture 3"
author: "Matthew E. Aiello-Lammens"
output:
  html_document:
    toc: yes
---

## Probability distributions

Fact: there is a lot of uncertainty associated with biological data.

### Uncertainty in biological data

There are two types of uncertainty:

* **Process uncertainty:** A result of our imperfect knowledge of things.
Example: we may get two different mean values for petal length for a particular Iris species if we go out to a field and measure two different sets of 50 flowers.

* **Observation uncertainty:** Inaccuracies resulting during measurement. 
Example: our petal lengths values may be "off" if we used two different rulers, which were not exactly the same.

We will focus more on **Process uncertainty**, than on **Observation uncertainty** in this class.
**We try to understand uncertainty and uncertain outcomes by using probability distributions.**

### Histograms and density plots as probability distributions

Let's go back to the plots of petal length. 
**Recall that when considering density plots the area under the curve or the area of the bins is equal to 1.**

```{r}
ggplot() +
  geom_histogram(data = iris_full, 
                 aes(x = Petal.Length, y = ..density.., fill = Species)) +
  facet_grid( Species ~ . ) +
  geom_density(data = iris_full, aes(x = Petal.Length, colour = Species)) +
  theme_bw()  
```

**Key idea: We can think of the area for a particular bin as the probability of getting a value that falls into that bin.**

#### Challange

Draw a probability distribution for throwing a single coin.

### Major Challange

Simulate a probability distribution for the number of heads you get when you throw 100 coins.

```{r}
all_throws <- c()

for( ind in 1:1000){
  ## Throw 100 coins
  throw <- sample( x = c("heads", "tails"), size = 100, replace = TRUE, prob = c(0.5, 0.5))
  
  ## Count the number of heads
  heads <- sum(throw == "heads")
  
  ## Record the number of heads
  all_throws <- c(all_throws, heads)

}

ggplot( data = NULL, aes(x = all_throws)) +
  geom_histogram(aes(y = ..density..) ) +
  theme_bw()
```


### Emperical distributions versus defined probability distributions

There are *many* defined probability distributions that have specific properties.
It is beyond the scope of this week's class to discuss the specific statistical properties of distributions, but for today know that:

* The area under the curve, or cumulative area of the bins is equal to 1
* Different values of the variable described by a distribution are on the x-axis
* The corresponding probability value for that particularl variable value is on the y-axis (or expressed by the total area of the bin in a histogram plot).

### Draw the probability distribution for the M&M draw

```{r}
## Colors as a vector
mm_colors <- c("blue", "brown", "green", "orange", "red", "yellow")
## Proportion/probability of each color
mm_probs <- c(.24, .14, .16, .20, .13, .14)
```

## Probability distributions

Many, but not all, of the distributions that we have been
introduced to this semester have fairly intuitive verbal 
descritpions of what kind of questions we may be trying to answer
when using that description.  Knowing these questions helps me when I am 
thinking about probability and statistics, so I thought I would
point them out. Similar descriptions can be found in 
Quinn and Keough 2002 and Bolker 2007, as well as many other
good texts.

As you go through this, be mindful that some of my notation
may be different than what we have seen so far in class.


### Normal Distribution:

### Log-normal Distribution:


### Bernoulli Distribution:

The Bernoulli distribution is a very simple distribution that
can be used if we have a single event (or experiment)
that has two possible outcomes, governed by some probability. 
For example, the probability of the outcome of a 
single coin toss with a fair coin 
can be described using a Bernoulli distribution.

${pdf} = P(x) = h$ if $x=1$ and $(1-h)$ if $x=0$, where
$x$ is the event outcome (i.e., heads or tails)

### Binomial Distribution:

Recall (or check the figure adapted from Leemis 1986) that 
the Binomial distribution can be thought of as the 
sum of $n$ Bernoulli distributions, all with the same 
parameterization (i.e., $h$ from above).  This is useful
if we want to find the probability of getting a certain number
of successes if your repeat some experiment many times.

$$
{pdf} = P(x | N,h) = \binom{N}{x} h^x (1-h)^{N-x}
$$

*Question:* What is the number of successes, $x$, in $N$ trials,
where the probability of a success is $h$

### Negative Binomial Distribution:

We briefly talked about this distribution in class. Thinking about how 
this distribution is related to the Binomial distribution, where as
for the latter setting the number of trials, $N$, as fixed, and asking
about the number of successes, $x$, for the **Negative** binomial we 
are setting the number of successes as fixed, $x$, and asking how many
trials, $N$, it takes to get that number of successes.

$$
{pdf} = P(N|x,h) = \binom{N-1}{x-1} h^x (1-h)^{N-x}
$$

*Question:* What is the number of trials, $N$, needed to reach the
$x$ th success.

### Geometric Distribution:

The Geometric distribution is a special case of the Negative
Binomial, where we are asking specifically about the number
of trials to reach the **first** success (i.e. the case of
the negative binomial where $x=1$.

$$
{pdf} = P(N|h) = h (1-h)^{N-1}
$$

*Question:* What is the number of trials, $N$, needed to reach the
1st success.

### Poisson Distribution:

There are many questions we may ask that are related
to the Poisson distribution. Usually we think of the 
Poisson when we have some process that usually results
in a small number most of the times and produces larger 
numbers less frequently. Think about the number of eggs produced
by some bird, or the number of off spring for some animal. 
By substituting the ususal $\lambda$ value in the Poisson 
with $\lambda T$, where $T$ is some defined time period, and 
$\lambda$ is some rate value, we can use the Poisson to address 
questions concerning the number of successes in some time 
period T.

$$
{pdf} = P(x|T,\lambda) = \frac{ e^{-(\lambda T)} (\lambda T)^x }{ x! }
$$



***

